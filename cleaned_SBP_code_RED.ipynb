{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# imports:"
      ],
      "metadata": {
        "id": "CUfd30Rpkac0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8lMgyqjkWb-"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForSequenceClassification\n",
        "# from transformers import TFAutoModelForSequenceClassification\n",
        "# from transformers import AutoTokenizer, AutoConfig\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import csv\n",
        "import urllib.request\n",
        "import pandas as pd # # data processing/manipulation\n",
        "# from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "# from detoxify import Detoxify\n",
        "tqdm.pandas()\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib notebook\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.model_selection\n",
        "import sklearn.preprocessing as preproc\n",
        "from sklearn.feature_extraction import text\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "%matplotlib inline\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "# Text embedding\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "# import emoji\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import svm\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import svm\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(31415)\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "np.random.seed(31415)\n",
        "torch.manual_seed(31415)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdxMQStHknwi",
        "outputId": "f3f75bb8-072c-4764-b084-efd702504cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc560e88330>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dXVaoDBknt1",
        "outputId": "6cd8a902-c862-4eea-ebc0-8721a47d4c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading files"
      ],
      "metadata": {
        "id": "2kEb6uJOkyib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Rastogi Reddit\")\n",
        "\n",
        "with open('/content/drive/MyDrive/HyperText/alternative datasets/3_16_rastogi_reddit_POST_1646.pickle', 'rb') as f:\n",
        "    post_sbert = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/HyperText/alternative datasets/3_16_rastogi_reddit_POST_7EMO_1646.pickle', 'rb') as f:\n",
        "    post_7Emo_ft = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/HyperText/alternative datasets/3_16_rastogi_reddit_POST_28EMO_1646.pickle', 'rb') as f:\n",
        "    post_goEmo_28FT = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/HyperText/alternative datasets/3_16_use_rastogi_reddit_comments_7EMO_1646.pickle', 'rb') as f:\n",
        "    comments_7Emo_ft = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/HyperText/alternative datasets/3_16_rastogi_reddit_comments_28EMO_1646.pickle', 'rb') as f:\n",
        "    comments_goEmo_28FT = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/HyperText/alternative datasets/3_16_rastogi_reddit_POST_1646_LABELS.pickle', 'rb') as f:\n",
        "    lebels = pickle.load(f)\n",
        "\n",
        "\n",
        "\n",
        "# print(\"dreadit\")\n",
        "# with open('/content/drive/MyDrive/HyperText/PKLz/26_go_emo_6emos_14ft_allComments_/sbert_embedding/Sbert_post_embed26_distilbert-base-cased.pickle', 'rb') as f:\n",
        "#     post_sbert = pickle.load(f)\n",
        "# with open('/content/drive/MyDrive/HyperText/PKLz/26_go_emo_6emos_14ft_allComments_/all_embed_post26_hartmann_emo.pickle', 'rb') as f:\n",
        "#     post_7Emo_ft = pickle.load(f)\n",
        "# with open('/content/drive/MyDrive/HyperText/PKLz/26_go_emo_6emos_14ft_allComments_/all_embed_post26_go_emo.pickle', 'rb') as f:\n",
        "#     post_goEmo_28FT = pickle.load(f)\n",
        "\n",
        "# with open('/content/drive/MyDrive/HyperText/PKLz/26_go_emo_6emos_14ft_allComments_/all_embed_comments26_hartmann_emo.pickle', 'rb') as f:\n",
        "#     comments_7Emo_ft = pickle.load(f)\n",
        "# with open('/content/drive/MyDrive/HyperText/PKLz/26_go_emo_6emos_14ft_allComments_/all_embed_comments26_go_emo.pickle', 'rb') as f:\n",
        "#     comments_goEmo_28FT = pickle.load(f)\n",
        "\n",
        "# with open('/content/drive/MyDrive/HyperText/PKLz/dreadit_2420_labels_list.pickle', 'rb') as f:\n",
        "#   #/content/drive/MyDrive/HyperText/PKLz/dreadit_2420_labels_list.pickle\n",
        "#   #/content/drive/MyDrive/HyperText/PKLz/dreadit_2420_labels_list.pickle\n",
        "#     lebels = pickle.load(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nEgm07Lknk-",
        "outputId": "2e31fdba-2f9b-4d45-d5d7-ee7613ee7d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rastogi Reddit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZWtvIeUakyOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "#our  work OUR special model\n",
        "y = []\n",
        "\n",
        "posts_emo = []\n",
        "posts_bert = []\n",
        "\n",
        "comments =[]\n",
        "\n",
        "labels=[]\n",
        "\n",
        "\n",
        "from statistics import mode, median\n",
        "import numpy as np\n",
        "\n",
        "for post_id in post_sbert.keys():\n",
        "  bert_emb_feat = post_sbert[post_id] # Bert embedding         #first we have the embeddings\n",
        "  #emo_feat = np.array(post_goEmo_28FT[post_id])#[0]  #[0] for rastogi only\n",
        "  #print(emo_feat)\n",
        "  emo_feat = np.array(post_7Emo_ft[post_id][0])#[0] for rastogi only\n",
        "  #emo_feat = np.array(post_7Emo_ft[post_id])# for dreadit\n",
        "  #print(emo_feat)\n",
        "\n",
        "\n",
        "  y_post = lebels[post_id]\n",
        "  #print(y_post)\n",
        "\n",
        "  #emo_comments_features = np.array(comments_goEmo_28FT[post_id][:150]) #comments_7Emo_ft #comments_goEmo_28FT\n",
        "  emo_comments_features = np.array(comments_7Emo_ft[post_id][:150])\n",
        "\n",
        "  posts_emo.append(emo_feat)\n",
        "  posts_bert.append(bert_emb_feat)\n",
        "  #print(posts_emo)\n",
        " # print(posts_bert)\n",
        "\n",
        "  comments.append(emo_comments_features)\n",
        "  labels.append(y_post)\n",
        "  #print(labels)\n",
        "\n",
        "  Emo_gap = []\n",
        "  for vec in emo_comments_features:\n",
        "      result = abs(vec - emo_feat)\n",
        "      Emo_gap.append(result)\n",
        "\n",
        "  num_zeros_lists = 150 - len(Emo_gap)\n",
        "  filled_emo_vector=np.zeros((num_zeros_lists, len(Emo_gap[0])), dtype=float)\n",
        "\n",
        "  Emo_gap.extend(filled_emo_vector.tolist())\n",
        "  #print(Emo_gap)\n",
        "\n",
        "  Emo_gap = np.array(Emo_gap)\n",
        "  comments.append(Emo_gap)\n",
        "  Emo_gap = Emo_gap.reshape(-1)#flattening here\n",
        "\n",
        "  X.append(np.concatenate((Emo_gap, bert_emb_feat)))\n",
        "  #X.append(Emo_gap)\n",
        "  y.append(y_post)"
      ],
      "metadata": {
        "id": "LmGPlI8qkyLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=31415)"
      ],
      "metadata": {
        "id": "taN2AgvkkyJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "bnTzEwTblScJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#3737373737\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, num_of_emo, num_of_comments, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        torch.manual_seed(600)\n",
        "\n",
        "\n",
        "        self.input_size = num_of_emo * num_of_comments\n",
        "        print(input_size)\n",
        "\n",
        "        self.bert_start_index = 150 * num_of_emo\n",
        "        print(self.bert_start_index )\n",
        "\n",
        "\n",
        "        # Emotion\n",
        "        self.fc1 = nn.Linear(self.input_size, 700)#1000 was 2000\n",
        "        self.fc2 = nn.Linear(700, 500)\n",
        "        self.fc3 = nn.Linear(500, 100)\n",
        "\n",
        "\n",
        "        # Bert + Emo\n",
        "        self.fc4 = nn.Linear(768 + 100, 400)\n",
        "        self.fc5 = nn.Linear(400, 200)\n",
        "        self.fc6 = nn.Linear(200, 100)\n",
        "        self.fc7 = nn.Linear(100, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        x_emo = x[:, :self.input_size]\n",
        "\n",
        "        x_bert = x[:, self.bert_start_index:]\n",
        "\n",
        "\n",
        "        # Emotion\n",
        "        x_emo = F.relu(self.fc1(x_emo))\n",
        "        x_emo = F.relu(self.fc2(x_emo))\n",
        "        x_emo = self.fc3(x_emo)\n",
        "\n",
        "\n",
        "\n",
        "        # Bert + x_emo\n",
        "        x_bert = torch.concat((x_bert, x_emo), dim=1)\n",
        "\n",
        "\n",
        "\n",
        "        x_bert = F.relu(self.fc4(x_bert))\n",
        "        x_bert = F.relu(self.fc5(x_bert))\n",
        "        x_bert = F.relu(self.fc6(x_bert))\n",
        "\n",
        "        out = self.fc7(x_bert)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Load the BERT embeddings and binary labels\n",
        "###### #### training ################\n",
        "bert_embeddings_training = X_train#X_Emo_train#X_train # load your BERT embeddings here #X_bert_train  #X_Emo_train\n",
        "bert_embeddings_training = np.array(bert_embeddings_training, dtype=np.float32)\n",
        "labels_training = y_train# load your binary labels here\n",
        "\n",
        "# Convert the data to tensors\n",
        "bert_embeddings_training = torch.tensor(bert_embeddings_training)\n",
        "labels_training = torch.tensor(labels_training).long()\n",
        "########## testing ################\n",
        "bert_embeddings_testing = X_test#| X_Emo_test#X_test # load your BERT embeddings here #X_bert_test #X_Emo_test\n",
        "bert_embeddings_testing = np.array(bert_embeddings_testing, dtype=np.float32)\n",
        "labels_testing = y_test# load your binary labels here\n",
        "\n",
        "\n",
        "# Convert the data to tensors\n",
        "bert_embeddings_training = torch.tensor(bert_embeddings_training)\n",
        "labels_training = torch.tensor(labels_training).long().view(-1)\n",
        "bert_embeddings_testing = torch.tensor(bert_embeddings_testing)\n",
        "labels_testing = torch.tensor(labels_testing).long().view(-1)\n",
        "\n",
        "\n",
        "# Define the model parameters\n",
        "input_size = bert_embeddings_training.shape[1]\n",
        "#print(\"input_size is \",input_size)\n",
        "hidden_size = 256#128\n",
        "num_classes = 2\n",
        "\n",
        "# Initialize the model\n",
        "num_of_emo = 7\n",
        "num_of_comments = 150\n",
        "\n",
        "model = MLP(num_of_emo, num_of_comments, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "num_of_epochs = 32\n",
        "\n",
        "for epoch in range(num_of_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(bert_embeddings_training)\n",
        "    loss = criterion(outputs, labels_training)\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 2 == 0:\n",
        "        print(f'Epoch [{epoch+1}], Loss: {loss.item()}')\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(bert_embeddings_training)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    acc_train = accuracy_score(labels_training, predicted)\n",
        "    precision_train = precision_score(labels_training, predicted)\n",
        "    recall_train = recall_score(labels_training, predicted)\n",
        "    f1_train = f1_score(labels_training, predicted)\n",
        "\n",
        "    print(f'Training set metrics:')\n",
        "    print(f'Accuracy: {acc_train * 100}%')\n",
        "    print(f'F1 Score: {f1_train}')\n",
        "    print(f'Precision: {precision_train}')\n",
        "    print(f'Recall: {recall_train}')\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs1 = model(bert_embeddings_testing)\n",
        "    _, predicted1 = torch.max(outputs1.data, 1)\n",
        "\n",
        "    acc_test = accuracy_score(labels_testing, predicted1)\n",
        "    precision_test = precision_score(labels_testing, predicted1)\n",
        "    recall_test = recall_score(labels_testing, predicted1)\n",
        "    f1_test = f1_score(labels_testing, predicted1)\n",
        "\n",
        "    print(f'\\nTest set metrics:')\n",
        "    print(f'Accuracy: {acc_test * 100}%')\n",
        "    print(f'F1 Score: {f1_test}')\n",
        "    print(f'Precision: {precision_test}')\n",
        "    print(f'Recall: {recall_test}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2mYqiUzykyGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sf5Yul2EkyD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iKnont2E_h2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-VLTxBfIzpgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DJLqwMoa0VuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}